{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Transformer: causal decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "A reimplementation of Andrej Karpahty's GPT from scratch [lecture](https://www.youtube.com/watch?v=kCc8FmEb1nY).\n",
    "\n",
    "Re-written for learning purposes.\n",
    "\n",
    "Also, Drake lyrics instead of Shakespeare so we can accelerate the birth of Drake AI."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read Drake lyrics into memory\n",
    "with open('../../data/drake_lyrics.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(f\"{len(text):,}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Preview text\n",
    "print(text[:1000])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create character vocabulary and embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up token encoder and decoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "s2i = { ch:i for i,ch in enumerate(chars) }\n",
    "i2s = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [s2i[c] for c in s]\n",
    "decode = lambda l: ''.join([i2s[i] for i in l])\n",
    "\n",
    "print(encode(\"hello world\"))\n",
    "print(decode(encode(\"hello world\")))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create token stream (`data: Tensor`) from encoded text (tokens)\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "block_size = 8  # max context length for predictions\n",
    "batch_size = 4  # num parallel sequences to process\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_interval = 300\n",
    "eval_iters = 200\n",
    "max_iters = 5000\n",
    "n_heads = 4\n",
    "n_block_layers = 12"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare training data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split data into training and validation set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n = int(.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(len(train_data))\n",
    "print(len(val_data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# X is training inputs\n",
    "# Y is target outputs (labels)\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"{context.tolist()} -> {target}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up batch function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_ixs = torch.randint(len(data) - block_size, (4,))\n",
    "x = torch.stack([data[i:i+block_size] for i in batch_ixs])\n",
    "print(f\"{x.shape=}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Introduce a batch dimension\n",
    "\n",
    "torch.manual_seed(11235)\n",
    "batch_size = 4 # num of independent sequences to process in each forward and backward pass of the Transformer (in parallel)\n",
    "# block_size = 8 # max context length to make predictions\n",
    "\n",
    "def get_batch(is_training =True):\n",
    "    # row length = block_size\n",
    "    # number of rows (height) = batch_size\n",
    "    # generate a small batch of data of inputs `x` and targets `y`\n",
    "    batch_data = train_data if is_training else val_data\n",
    "\n",
    "    # e.g. ix=tensor([ 76049, 234249, 934904, 560986])\n",
    "    ix = torch.randint(len(batch_data) - block_size, (batch_size,))\n",
    "\n",
    "    # add\n",
    "    x = torch.stack([batch_data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([batch_data[i+1:i+block_size+1] for i in ix])\n",
    "    return x,y\n",
    "\n",
    "xb, yb = get_batch(is_training=True)\n",
    "print('# INPUT BATCH [X]')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('# TARGET BATCH [Y]')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size): # batch dim\n",
    "    for t in range(block_size): # time dim\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"{context.tolist()} -> {target}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Scaled dot-product attention (dry run)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# single head self-attention\n",
    "head_size = 16 # number of features (H)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# Every node (token) emits two vectors:\n",
    "# 1. query (Q) - what a token is looking for\n",
    "# 2. key (k) - what a token is\n",
    "q = query(x) # (B, T, H)\n",
    "k = key(x)   # (B, T, H)\n",
    "\n",
    "# Transpose the last two dimensions of k -> (B, H, T)\n",
    "# (B, T, H) @ (B, H, T) -> (B, T, T)\n",
    "ws = q @ k.transpose(-2, -1)\n",
    "\n",
    "# only pay attention to preceding tokens\n",
    "tril = torch.tril(torch.ones(T, T)) # (T, T)\n",
    "# zeroes mask (T, T) broadcast through weights (B, T, T)\n",
    "ws = ws.masked_fill(tril == 0, float('-inf')) # (B, T, T)\n",
    "# normalise\n",
    "ws = F.softmax(ws, dim=-1) # (B, T, T)\n",
    "print(f\"softmaxed: {ws=}\")\n",
    "\n",
    "# the content that a token has to offer\n",
    "v = value(x) # (B, T, H)\n",
    "\n",
    "# weighted aggregation of past tokens\n",
    "out = ws @ v # (B, T, H)\n",
    "\n",
    "print(f\"{v=}\")\n",
    "print(f\"{out[0]=}\")\n",
    "\n",
    "# attention weights of tokens in first sequence\n",
    "# ws[0] # (T, T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Single head of self attention"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"A head of self attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size: int):\n",
    "        super().__init__()\n",
    "        # Initialise Q, K, V\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        # the mask doesn't get updated during backpropagation (optimizer step)\n",
    "        # but is part of the state of the model, so saved and loaded with the model\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B, T, C = x.shape\n",
    "        q = self.query(x) # (B, T, C)\n",
    "        k = self.key(x)   # (B, T, C)\n",
    "\n",
    "        # compute attention scores (\"affinities\")\n",
    "        ws = q @ k.transpose(-2, -1) # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        ws = ws.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "\n",
    "        # compute relative predictions\n",
    "        ws = F.softmax(ws, dim=-1) # (B, T, T)\n",
    "\n",
    "        v = self.value(x) # (B, T, C)\n",
    "        out = ws @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Multi-head self-attention"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple heads of self-attention concatenated.\n",
    "    Parallelized over the channel (C) dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads: int, head_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # initialise multiple self-attention heads each with `head_size` dimension\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "        # projection layer to transform concatenated outputs of all heads\n",
    "        # back to the original embedding dimension\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # concat outputs of each attention head\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "        # project outputs to input (embed) dimensions\n",
    "        out = self.proj(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Feed forward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple feed-forward NN with a single hidden layer\n",
    "    [Linear -> ReLU]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int):\n",
    "        super().__init__()\n",
    "        # scaling inner layer channel size by 4 is a hyperparameter recommendation in\n",
    "        # 3.3 Position-wise feed forward networks from Attention is All You Need\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), # projection\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transformer block"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Carries out self attention (communication) and feed forward (computation) on input embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, n_heads: int):\n",
    "        \"\"\"\n",
    "        :param n_embd: embedding dimension\n",
    "        :param n_heads: number of heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # self-attention heads\n",
    "        # the same 32 embedding dimensions are split across 4 heads\n",
    "        head_size: int = n_embd // n_heads\n",
    "        self.sa = MultiHead(n_heads, head_size)\n",
    "\n",
    "        # feed forward layer\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "        # both LayerNorm layers\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Applies a round of self-attention to embeddings.\n",
    "        :param x: the current embeddings with shape (B, T, C)\n",
    "        :return: output data following self-attention\n",
    "        \"\"\"\n",
    "\n",
    "        # apply self attention to normalised plus original x for residual connection\n",
    "        # using pre-norm formulation here (modern deviation from Attention is All You Need)\n",
    "        norm_x = self.ln1(x)\n",
    "        x = x + self.sa(norm_x)\n",
    "\n",
    "        # apply feed forward plus original x for residual connection\n",
    "        norm_x = self.ln2(x)\n",
    "        x = x + self.ffwd(norm_x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# number of embedding dimensions\n",
    "n_embd = 32\n",
    "\n",
    "class DecoderLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*(TransformerBlock(n_embd, n_heads=n_heads) for _ in range (n_block_layers)))\n",
    "        self.ln_f = nn.LayerNorm(n_embd), # last layer normalisation\n",
    "\n",
    "        # language-modelling head\n",
    "        # maps output of transformer block (self-attention and feed-forward layers) to the vocabulary\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idxs: torch.Tensor, targets=None):\n",
    "\n",
    "        B, T = idxs.shape\n",
    "\n",
    "        # Sum text and position information\n",
    "        tok_emb = self.token_embedding_table(idxs)                              # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "\n",
    "        # sum token embeddings with broadcast position embeddings, right-aligned as (1, T, C)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            # inference mode -> we don't need to calculate loss\n",
    "            loss = None\n",
    "        else:\n",
    "            # training mode -> we need to calculate loss\n",
    "\n",
    "            # negative log likelihood loss\n",
    "            # pytorch wants B x T x C\n",
    "            B, T, C = logits.shape\n",
    "            # print(f\"B, T, C = {logits.shape=}\")\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T) # or .view(-1)\n",
    "\n",
    "            # Compute loss between logit and target tensors\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idxs, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Inference.\n",
    "        Turns a (B, T) array of indices into (B, T+1), (B, T+2)... (B, T+N)\n",
    "        :param idx: a (B, T) array of indices in the current context\n",
    "        :param max_new_tokens:\n",
    "        :return: a (B, T+max_new_tokens) array of indices\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop tokens that fit into context\n",
    "            cropped_idxs = idxs[:, -block_size:]\n",
    "            # get predictions\n",
    "            logits, loss = self(cropped_idxs)\n",
    "            # take the logits for the last token in the current sequence\n",
    "            logits = logits[:, -1, :] # becomes (B, C), taking the last in the T dimension\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from distribution\n",
    "            next_ix = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idxs = torch.cat((idxs, next_ix), dim=1) # (B, T+1)\n",
    "        return idxs\n",
    "\n",
    "\n",
    "def test_model():\n",
    "    m = DecoderLanguageModel()\n",
    "    logits, loss = m(xb, yb)\n",
    "    print(f\"logits: {logits.shape}\")\n",
    "    print(loss.item())\n",
    "\n",
    "    context = torch.zeros([1, 1], dtype=torch.long, device=device)\n",
    "    print(decode(m.generate(context, max_new_tokens=100)[0].tolist()))\n",
    "\n",
    "\n",
    "test_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepare to estimate loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            X, Y = get_batch(is_training=split == 'train')\n",
    "            logits, loss = m(X, Y)\n",
    "            losses[i] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(model: nn.Module, train_steps=10_000):\n",
    "    loss_values = []\n",
    "    loss = None\n",
    "    for step in range(train_steps):\n",
    "\n",
    "        # Sample a batch of data for training. `xb` contains the input sequences and `yb` contains the target sequences.\n",
    "        xb, yb = get_batch(is_training=True)\n",
    "\n",
    "        # Forward pass: Compute predictions (logits) and loss for this batch using the model.\n",
    "        # loss= L(yb, logits) where L is loss function (cross-entropy)\n",
    "        logits, loss = model(xb, yb)\n",
    "\n",
    "        # Before computing the gradients, we need to zero out the gradients from the previous step.\n",
    "        # This is because PyTorch accumulates gradients on subsequent backward passes.\n",
    "        # Set gradients of all params to zero: ∇params = 0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Backward pass: Compute the gradients of the loss with respect to the model's parameters.\n",
    "        # ∇params = d(loss)/d(params)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model's parameters using the computed gradients.\n",
    "        # params' = params - learning_rate * ∇params\n",
    "        optimizer.step()\n",
    "\n",
    "        # store the detached loss value for this step\n",
    "        loss_values.append(loss.detach().item())\n",
    "\n",
    "        # print loss at intervals\n",
    "        if step % 500 == 0:\n",
    "            estimated_loss = estimate_loss()\n",
    "            print(f\"step {step} – training loss: {estimated_loss['train']} | val loss: {estimated_loss['val']}\")\n",
    "\n",
    "    # print loss for the last batch\n",
    "    # not necessarily the average or total loss for all batches.\n",
    "    if loss is not None:\n",
    "        print(f\"loss after {train_steps} training steps: {loss.detach().item()}\")\n",
    "    return loss_values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = DecoderLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# Initialize the optimiser with the parameters of the model `m` and a learning rate of 1e-3.\n",
    "# AdamW is a variant of the Adam optimiser that has weight decay regularization.\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) # learning rate of 1e-4 for bigger NNs\n",
    "\n",
    "train_steps = 10_000\n",
    "loss_values = train(model=m, train_steps=train_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analyse loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def smooth(values, beta):\n",
    "    smoothed = []\n",
    "    for v in values:\n",
    "        if smoothed:\n",
    "            previous = smoothed[-1]\n",
    "            smoothed.append(previous * beta + (1 - beta) * v)\n",
    "        else:\n",
    "            smoothed.append(v)\n",
    "    return smoothed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_colour = '#1f77b4'\n",
    "\n",
    "# plot loss values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_values, label=f\"{train_steps} steps\", color=plot_colour, alpha=.2)\n",
    "plt.plot(smooth(loss_values, 0.995), label=f\"{train_steps} steps\", color=plot_colour)\n",
    "plt.xlabel('training step')\n",
    "plt.ylabel('loss')\n",
    "plt.title('loss vs training steps')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.tensor(encode(\"I'm from the \"), dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.zeros([1, 1], dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idx = torch.tensor([encode(\"You used to call me on my \")], dtype=torch.long) # we're going to need more Drake albums\n",
    "print(decode(m.generate(idx, max_new_tokens=300)[0].tolist()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.tensor(encode(\"I'm from the \"), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.zeros([1, 1], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "idx = torch.tensor([encode(\"You used to call me on my \")], dtype=torch.long) # we're going to need more Drake albums\n",
    "print(decode(m.generate(idx, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([37,  7, 70,  1, 63, 75, 72, 70,  1, 77, 65, 62,  1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(encode(\"I'm from the \"), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros([1, 1], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You used to call me on my lone\n",
      "E'It goinf\n",
      "Girls\n",
      "Sound\n",
      "And the slike 11: Drake]\n",
      "But anyy\n",
      "\n",
      "[Putro: Drake you Godeach\n",
      "With neds poplies whit\n",
      ", yeah, it's vechack's)\n",
      "Nowe]\n",
      "Appich, Homes me all never becone, svilly hobuld it's when I'm felus?\n",
      "They Bare\n",
      "\n",
      "[Verse 1: Eem the giivice, you\"\"\n",
      "\n",
      "[Ponerst\"\n",
      "I'm niggre it I onees be uppink\n",
      "B\n"
     ]
    }
   ],
   "source": [
    "idx = torch.tensor([encode(\"You used to call me on my \")], dtype=torch.long) # we're going to need more Drake albums\n",
    "print(decode(m.generate(idx, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for param in m.parameters():\n",
    "#     print(f\"{type(param)=} - {param=}: {param.size()=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Expected loss (without training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "print(f\"{-math.log(1/len(chars))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}