{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Transformer: causal decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A re-implementation of Andrej Karpahty's GPT from scratch [lecture](https://www.youtube.com/watch?v=kCc8FmEb1nY).\n",
    "\n",
    "I rewrote it for my own learning. Titles, comments and variables names are focused on conciseness and clarity.\n",
    "\n",
    "Also, Drake lyrics instead of Shakespeare so we can to accelerate the birth of Drake AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1127970b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770,711\n"
     ]
    }
   ],
   "source": [
    "# Read Drake lyrics into memory\n",
    "with open('../data/drake_lyrics.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(f\"{len(text):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"[Verse]\n",
      "Put my feelings on ice\n",
      "Always been a gem\n",
      "Certified lover boy, somehow still heartless\n",
      "Heart is only gettin' colder\"\n",
      "\"[Verse]\n",
      "Hands are tied\n",
      "Someone's in my ear from the other side\n",
      "Tellin' me that I should pay you no mind\n",
      "Wanted you to not be with me all night\n",
      "Wanted you to not stay with me all night\n",
      "I know, you know, who that person is to me\n",
      "Doesn't really change things\n",
      "\n",
      "[Chorus]\n",
      "I know you're scared of dating, falling for me\n",
      "Shorty, surely you know me\n",
      "Right here for you always\n",
      "You know, I don't ever change\n",
      "Right here for you always\n",
      "You know I don't ever change\n",
      "Right here for you\n",
      "\n",
      "[Bridge]\n",
      "In mind you make me want to do things, love you\n",
      "Like I'm supposed to\n",
      "You make me want to love you\n",
      "Like I'm supposed to\n",
      "You make me want to love you\n",
      "Like I'm supposed to, remind you\n",
      "Ayy\n",
      "\n",
      "[Chorus]\n",
      "I know you're scared of dating, falling for me\n",
      "Shorty, by now you know me\n",
      "Right here for you always\n",
      "You know, I don't ever change\n",
      "Right here for you always\n",
      "You know I don't ever change\n",
      "Right here for\n"
     ]
    }
   ],
   "source": [
    "# Preview text\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create character vocabulary and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"$%&'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz{|}àáèéëñóú –—‘’“”… \n",
      "104\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Set up token encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65, 62, 69, 69, 72, 1, 80, 72, 75, 69, 61]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "s2i = { ch:i for i,ch in enumerate(chars) }\n",
    "i2s = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [s2i[c] for c in s]\n",
    "decode = lambda l: ''.join([i2s[i] for i in l])\n",
    "\n",
    "print(encode(\"hello world\"))\n",
    "print(decode(encode(\"hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([770711]) torch.int64\n",
      "tensor([ 3, 55, 50, 62, 75, 76, 62, 56,  0, 44, 78, 77,  1, 70, 82,  1, 63, 62,\n",
      "        62, 69, 66, 71, 64, 76,  1, 72, 71,  1, 66, 60, 62,  0, 29, 69, 80, 58,\n",
      "        82, 76,  1, 59, 62, 62, 71,  1, 58,  1, 64, 62, 70,  0, 31, 62, 75, 77,\n",
      "        66, 63, 66, 62, 61,  1, 69, 72, 79, 62, 75,  1, 59, 72, 82, 12,  1, 76,\n",
      "        72, 70, 62, 65, 72, 80,  1, 76, 77, 66, 69, 69,  1, 65, 62, 58, 75, 77,\n",
      "        69, 62, 76, 76,  0, 36, 62, 58, 75, 77,  1, 66, 76,  1, 72, 71, 69, 82,\n",
      "         1, 64, 62, 77, 77, 66, 71,  7,  1, 60, 72, 69, 61, 62, 75,  3,  0,  3,\n",
      "        55, 50, 62, 75, 76, 62, 56,  0, 36, 58, 71, 61, 76,  1, 58, 75, 62,  1,\n",
      "        77, 66, 62, 61,  0, 47, 72, 70, 62, 72, 71, 62,  7, 76,  1, 66, 71,  1,\n",
      "        70, 82,  1, 62, 58, 75,  1, 63, 75, 72, 70,  1, 77, 65, 62,  1, 72, 77,\n",
      "        65, 62, 75,  1, 76, 66, 61, 62,  0, 48, 62, 69, 69, 66, 71,  7,  1, 70,\n",
      "        62,  1, 77, 65, 58, 77,  1, 37,  1, 76, 65, 72, 78, 69, 61,  1, 73, 58,\n",
      "        82,  1, 82, 72, 78,  1, 71, 72,  1, 70, 66, 71, 61,  0, 51, 58, 71, 77,\n",
      "        62, 61,  1, 82, 72, 78,  1, 77, 72,  1, 71, 72, 77,  1, 59, 62,  1, 80,\n",
      "        66, 77, 65,  1, 70, 62,  1, 58, 69, 69,  1, 71, 66, 64, 65, 77,  0, 51,\n",
      "        58, 71, 77, 62, 61,  1, 82, 72, 78,  1, 77, 72,  1, 71, 72, 77,  1, 76,\n",
      "        77, 58, 82,  1, 80, 66, 77, 65,  1, 70, 62,  1, 58, 69, 69,  1, 71, 66,\n",
      "        64, 65, 77,  0, 37,  1, 68, 71, 72, 80, 12,  1, 82, 72, 78,  1, 68, 71,\n",
      "        72, 80, 12,  1, 80, 65, 72,  1, 77, 65, 58, 77,  1, 73, 62, 75, 76, 72,\n",
      "        71,  1, 66, 76,  1, 77, 72,  1, 70, 62,  0, 32, 72, 62, 76, 71,  7, 77,\n",
      "         1, 75, 62, 58, 69, 69, 82,  1, 60, 65, 58, 71, 64, 62,  1, 77, 65, 66,\n",
      "        71, 64, 76,  0,  0, 55, 31, 65, 72, 75, 78, 76, 56,  0, 37,  1, 68, 71,\n",
      "        72, 80,  1, 82, 72, 78,  7, 75, 62,  1, 76, 60, 58, 75, 62, 61,  1, 72,\n",
      "        63,  1, 61, 58, 77, 66, 71, 64, 12,  1, 63, 58, 69, 69, 66, 71, 64,  1,\n",
      "        63, 72, 75,  1, 70, 62,  0, 47, 65, 72, 75, 77, 82, 12,  1, 76, 78, 75,\n",
      "        62, 69, 82,  1, 82, 72, 78,  1, 68, 71, 72, 80,  1, 70, 62,  0, 46, 66,\n",
      "        64, 65, 77,  1, 65, 62, 75, 62,  1, 63, 72, 75,  1, 82, 72, 78,  1, 58,\n",
      "        69, 80, 58, 82, 76,  0, 53, 72, 78,  1, 68, 71, 72, 80, 12,  1, 37,  1,\n",
      "        61, 72, 71,  7, 77,  1, 62, 79, 62, 75,  1, 60, 65, 58, 71, 64, 62,  0,\n",
      "        46, 66, 64, 65, 77,  1, 65, 62, 75, 62,  1, 63, 72, 75,  1, 82, 72, 78,\n",
      "         1, 58, 69, 80, 58, 82, 76,  0, 53, 72, 78,  1, 68, 71, 72, 80,  1, 37,\n",
      "         1, 61, 72, 71,  7, 77,  1, 62, 79, 62, 75,  1, 60, 65, 58, 71, 64, 62,\n",
      "         0, 46, 66, 64, 65, 77,  1, 65, 62, 75, 62,  1, 63, 72, 75,  1, 82, 72,\n",
      "        78,  0,  0, 55, 30, 75, 66, 61, 64, 62, 56,  0, 37, 71,  1, 70, 66, 71,\n",
      "        61,  1, 82, 72, 78,  1, 70, 58, 68, 62,  1, 70, 62,  1, 80, 58, 71, 77,\n",
      "         1, 77, 72,  1, 61, 72,  1, 77, 65, 66, 71, 64, 76, 12,  1, 69, 72, 79,\n",
      "        62,  1, 82, 72, 78,  0, 40, 66, 68, 62,  1, 37,  7, 70,  1, 76, 78, 73,\n",
      "        73, 72, 76, 62, 61,  1, 77, 72,  0, 53, 72, 78,  1, 70, 58, 68, 62,  1,\n",
      "        70, 62,  1, 80, 58, 71, 77,  1, 77, 72,  1, 69, 72, 79, 62,  1, 82, 72,\n",
      "        78,  0, 40, 66, 68, 62,  1, 37,  7, 70,  1, 76, 78, 73, 73, 72, 76, 62,\n",
      "        61,  1, 77, 72,  0, 53, 72, 78,  1, 70, 58, 68, 62,  1, 70, 62,  1, 80,\n",
      "        58, 71, 77,  1, 77, 72,  1, 69, 72, 79, 62,  1, 82, 72, 78,  0, 40, 66,\n",
      "        68, 62,  1, 37,  7, 70,  1, 76, 78, 73, 73, 72, 76, 62, 61,  1, 77, 72,\n",
      "        12,  1, 75, 62, 70, 66, 71, 61,  1, 82, 72, 78,  0, 29, 82, 82,  0,  0,\n",
      "        55, 31, 65, 72, 75, 78, 76, 56,  0, 37,  1, 68, 71, 72, 80,  1, 82, 72,\n",
      "        78,  7, 75, 62,  1, 76, 60, 58, 75, 62, 61,  1, 72, 63,  1, 61, 58, 77,\n",
      "        66, 71, 64, 12,  1, 63, 58, 69, 69, 66, 71, 64,  1, 63, 72, 75,  1, 70,\n",
      "        62,  0, 47, 65, 72, 75, 77, 82, 12,  1, 59, 82,  1, 71, 72, 80,  1, 82,\n",
      "        72, 78,  1, 68, 71, 72, 80,  1, 70, 62,  0, 46, 66, 64, 65, 77,  1, 65,\n",
      "        62, 75, 62,  1, 63, 72, 75,  1, 82, 72, 78,  1, 58, 69, 80, 58, 82, 76,\n",
      "         0, 53, 72, 78,  1, 68, 71, 72, 80, 12,  1, 37,  1, 61, 72, 71,  7, 77,\n",
      "         1, 62, 79, 62, 75,  1, 60, 65, 58, 71, 64, 62,  0, 46, 66, 64, 65, 77,\n",
      "         1, 65, 62, 75, 62,  1, 63, 72, 75,  1, 82, 72, 78,  1, 58, 69, 80, 58,\n",
      "        82, 76,  0, 53, 72, 78,  1, 68, 71, 72, 80,  1, 37,  1, 61, 72, 71,  7,\n",
      "        77,  1, 62, 79, 62, 75,  1, 60, 65, 58, 71, 64, 62,  0, 46, 66, 64, 65,\n",
      "        77,  1, 65, 62, 75, 62,  1, 63, 72, 75])\n"
     ]
    }
   ],
   "source": [
    "# Create token stream (`data: Tensor`) from encoded text (tokens)\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Set up hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "block_size = 8  # max context length for predictions\n",
    "batch_size = 4  # num parallel sequences to process\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_interval = 300\n",
    "eval_iters = 200\n",
    "max_iters = 5000\n",
    "n_heads = 4\n",
    "n_block_layers = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Split data into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "693639\n",
      "77072\n"
     ]
    }
   ],
   "source": [
    "n = int(.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(len(train_data))\n",
    "print(len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] -> 55\n",
      "[3, 55] -> 50\n",
      "[3, 55, 50] -> 62\n",
      "[3, 55, 50, 62] -> 75\n",
      "[3, 55, 50, 62, 75] -> 76\n",
      "[3, 55, 50, 62, 75, 76] -> 62\n",
      "[3, 55, 50, 62, 75, 76, 62] -> 56\n",
      "[3, 55, 50, 62, 75, 76, 62, 56] -> 0\n"
     ]
    }
   ],
   "source": [
    "# X is training inputs\n",
    "# Y is target outputs (labels)\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"{context.tolist()} -> {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Set up batch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "batch_ixs = torch.randint(len(data) - block_size, (4,))\n",
    "x = torch.stack([data[i:i+block_size] for i in batch_ixs])\n",
    "print(f\"{x.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# INPUT BATCH [X]\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 1, 77, 65, 62,  1, 73, 72, 68],\n",
      "        [63, 62,  1, 58, 71, 61,  1, 37],\n",
      "        [83,  1, 60, 58, 75,  1, 71, 72],\n",
      "        [ 1, 37,  1, 61, 72, 71,  7, 77]])\n",
      "# TARGET BATCH [Y]\n",
      "torch.Size([4, 8])\n",
      "tensor([[77, 65, 62,  1, 73, 72, 68, 62],\n",
      "        [62,  1, 58, 71, 61,  1, 37,  7],\n",
      "        [ 1, 60, 58, 75,  1, 71, 72, 77],\n",
      "        [37,  1, 61, 72, 71,  7, 77,  1]])\n",
      "[1] -> 77\n",
      "[1, 77] -> 65\n",
      "[1, 77, 65] -> 62\n",
      "[1, 77, 65, 62] -> 1\n",
      "[1, 77, 65, 62, 1] -> 73\n",
      "[1, 77, 65, 62, 1, 73] -> 72\n",
      "[1, 77, 65, 62, 1, 73, 72] -> 68\n",
      "[1, 77, 65, 62, 1, 73, 72, 68] -> 62\n",
      "[63] -> 62\n",
      "[63, 62] -> 1\n",
      "[63, 62, 1] -> 58\n",
      "[63, 62, 1, 58] -> 71\n",
      "[63, 62, 1, 58, 71] -> 61\n",
      "[63, 62, 1, 58, 71, 61] -> 1\n",
      "[63, 62, 1, 58, 71, 61, 1] -> 37\n",
      "[63, 62, 1, 58, 71, 61, 1, 37] -> 7\n",
      "[83] -> 1\n",
      "[83, 1] -> 60\n",
      "[83, 1, 60] -> 58\n",
      "[83, 1, 60, 58] -> 75\n",
      "[83, 1, 60, 58, 75] -> 1\n",
      "[83, 1, 60, 58, 75, 1] -> 71\n",
      "[83, 1, 60, 58, 75, 1, 71] -> 72\n",
      "[83, 1, 60, 58, 75, 1, 71, 72] -> 77\n",
      "[1] -> 37\n",
      "[1, 37] -> 1\n",
      "[1, 37, 1] -> 61\n",
      "[1, 37, 1, 61] -> 72\n",
      "[1, 37, 1, 61, 72] -> 71\n",
      "[1, 37, 1, 61, 72, 71] -> 7\n",
      "[1, 37, 1, 61, 72, 71, 7] -> 77\n",
      "[1, 37, 1, 61, 72, 71, 7, 77] -> 1\n"
     ]
    }
   ],
   "source": [
    "# Introduce a batch dimension\n",
    "\n",
    "torch.manual_seed(11235)\n",
    "batch_size = 4 # num of independent sequences to process in each forward and backward pass of the Transformer (in parallel)\n",
    "# block_size = 8 # max context length to make predictions\n",
    "\n",
    "def get_batch(is_training =True):\n",
    "    # row length = block_size\n",
    "    # number of rows (height) = batch_size\n",
    "    # generate a small batch of data of inputs `x` and targets `y`\n",
    "    batch_data = train_data if is_training else val_data\n",
    "\n",
    "    # e.g. ix=tensor([ 76049, 234249, 934904, 560986])\n",
    "    ix = torch.randint(len(batch_data) - block_size, (batch_size,))\n",
    "\n",
    "    # add\n",
    "    x = torch.stack([batch_data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([batch_data[i+1:i+block_size+1] for i in ix])\n",
    "    return x,y\n",
    "\n",
    "xb, yb = get_batch(is_training=True)\n",
    "print('# INPUT BATCH [X]')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('# TARGET BATCH [Y]')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size): # batch dim\n",
    "    for t in range(block_size): # time dim\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"{context.tolist()} -> {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Scaled dot-product attention (dry run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmaxed: ws=tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [9.3721e-01, 6.2791e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [7.4353e-01, 2.2961e-01, 2.6857e-02, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [1.0209e-01, 2.5084e-02, 7.7802e-01, 9.4803e-02, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [1.3239e-02, 8.3880e-02, 4.5793e-02, 9.4440e-02, 7.6265e-01,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [2.5288e-02, 7.0410e-02, 7.1616e-01, 5.0822e-02, 1.0343e-01,\n",
      "          3.3890e-02, 0.0000e+00, 0.0000e+00],\n",
      "         [2.3808e-01, 9.2292e-02, 1.5704e-01, 1.8009e-01, 1.6292e-02,\n",
      "          1.3875e-01, 1.7745e-01, 0.0000e+00],\n",
      "         [9.7294e-01, 3.4147e-03, 1.7623e-03, 7.9526e-03, 5.9035e-03,\n",
      "          4.0651e-03, 1.7310e-03, 2.2285e-03]],\n",
      "\n",
      "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [5.5978e-01, 4.4022e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [2.7342e-01, 4.4766e-01, 2.7892e-01, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [2.0066e-01, 1.5204e-01, 2.8868e-01, 3.5861e-01, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [3.8487e-01, 1.3699e-01, 8.2439e-02, 2.7398e-01, 1.2172e-01,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [1.1851e-01, 1.2609e-01, 5.3316e-02, 1.8679e-02, 6.6357e-02,\n",
      "          6.1705e-01, 0.0000e+00, 0.0000e+00],\n",
      "         [2.5074e-02, 4.3786e-01, 2.1957e-02, 2.8271e-01, 9.9070e-02,\n",
      "          7.0648e-02, 6.2685e-02, 0.0000e+00],\n",
      "         [5.0256e-03, 4.1797e-03, 2.5898e-02, 2.1243e-01, 1.9348e-02,\n",
      "          2.8710e-03, 1.3689e-01, 5.9336e-01]],\n",
      "\n",
      "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [9.5221e-01, 4.7789e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [4.5900e-02, 7.9917e-02, 8.7418e-01, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [1.3232e-01, 3.8779e-01, 4.1373e-01, 6.6160e-02, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [1.7978e-02, 3.2616e-01, 5.3410e-01, 1.2160e-01, 1.7054e-04,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [1.1892e-01, 1.2587e-01, 9.6503e-03, 4.6982e-01, 2.6231e-01,\n",
      "          1.3422e-02, 0.0000e+00, 0.0000e+00],\n",
      "         [3.3560e-02, 1.8832e-01, 3.8704e-02, 4.5310e-02, 7.0091e-03,\n",
      "          6.5892e-01, 2.8174e-02, 0.0000e+00],\n",
      "         [3.6875e-02, 3.5258e-02, 9.7277e-02, 3.6420e-02, 3.5963e-01,\n",
      "          2.1721e-02, 1.0822e-02, 4.0200e-01]],\n",
      "\n",
      "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [9.3469e-01, 6.5314e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [2.3191e-01, 4.1231e-01, 3.5578e-01, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [2.3899e-01, 4.8733e-01, 1.2852e-01, 1.4517e-01, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [2.3472e-01, 2.5716e-01, 3.1310e-02, 1.4776e-01, 3.2905e-01,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [7.4430e-01, 7.7442e-02, 1.0939e-01, 6.1059e-03, 4.6234e-02,\n",
      "          1.6530e-02, 0.0000e+00, 0.0000e+00],\n",
      "         [1.0238e-01, 1.8936e-01, 1.6917e-01, 1.3038e-01, 4.6865e-02,\n",
      "          3.3959e-01, 2.2251e-02, 0.0000e+00],\n",
      "         [1.9231e-01, 1.0179e-01, 1.4451e-02, 2.3162e-01, 8.7426e-02,\n",
      "          1.9369e-01, 9.3197e-02, 8.5511e-02]]], grad_fn=<SoftmaxBackward0>)\n",
      "v=tensor([[[ 8.2471e-01, -3.3498e-01, -8.5473e-01,  1.4716e-01, -1.1222e+00,\n",
      "           7.3956e-01, -3.8025e-01,  1.1473e-02,  1.1573e-02,  4.2417e-01,\n",
      "          -1.0590e+00,  1.2839e+00, -2.8366e-01, -5.4395e-01,  2.2446e-01,\n",
      "           5.8767e-01],\n",
      "         [ 4.7282e-01,  3.5342e-01,  7.3463e-01, -2.1160e-01,  1.0172e+00,\n",
      "           3.3405e-01,  3.4105e-01, -1.3020e-02,  4.6410e-01,  4.6848e-01,\n",
      "           3.8330e-01, -7.2529e-01, -4.1046e-01,  9.0011e-01,  8.4183e-01,\n",
      "           1.5696e-01],\n",
      "         [ 2.5838e-01, -3.0040e-01, -3.6966e-01,  9.4080e-01, -1.7168e-01,\n",
      "          -5.6972e-01,  1.9151e-01, -5.5805e-01, -6.7172e-01, -1.2756e-01,\n",
      "           2.3539e-01, -1.2946e+00, -9.5998e-02, -1.1220e-02, -8.0066e-01,\n",
      "           1.5436e-02],\n",
      "         [ 3.1823e-01, -3.1544e-01, -7.4742e-01, -8.6198e-01, -4.6253e-01,\n",
      "          -3.6168e-01, -4.3968e-01,  3.0548e-02,  9.1988e-01,  4.5072e-01,\n",
      "          -2.6102e-01, -4.6472e-01, -2.9888e-01,  4.8518e-01, -1.8141e-01,\n",
      "           2.1372e-01],\n",
      "         [ 3.7712e-01,  9.1074e-01,  1.1028e+00,  4.2044e-01,  4.5940e-01,\n",
      "           8.4447e-01, -4.8823e-01, -4.9688e-01,  1.0398e-01, -3.3849e-02,\n",
      "          -8.3333e-01,  8.9185e-01,  3.6148e-01, -1.2628e-01, -5.6726e-01,\n",
      "          -3.9672e-01],\n",
      "         [ 2.5720e-01,  9.3159e-02, -1.4601e+00,  5.0127e-01, -9.6021e-01,\n",
      "          -4.7082e-01, -3.0788e-01, -3.5671e-01,  1.3841e-01, -7.7347e-01,\n",
      "          -8.6196e-01, -3.2056e-01,  6.3951e-02, -6.6390e-01, -6.9662e-01,\n",
      "           5.7845e-01],\n",
      "         [-7.9675e-01, -2.9309e-01, -6.3644e-01, -6.3774e-01, -3.5879e-02,\n",
      "          -9.2921e-02,  5.9627e-01,  1.0198e-01,  4.6966e-01, -2.5997e-01,\n",
      "           9.5776e-01, -2.3169e-01, -2.7806e-01,  8.2779e-01,  6.2074e-01,\n",
      "          -1.1848e+00],\n",
      "         [ 2.3142e-01,  8.4537e-01, -1.7845e+00,  7.1840e-01,  6.1923e-01,\n",
      "          -1.5340e-02, -7.0517e-01,  4.5810e-01,  5.3300e-01, -1.2446e+00,\n",
      "           1.6394e-01,  4.0765e-01,  1.0163e+00,  1.3399e+00,  2.7049e-01,\n",
      "          -1.1954e+00]],\n",
      "\n",
      "        [[ 2.9093e-01, -1.2214e-02, -1.5057e-01,  8.3747e-01, -6.0436e-01,\n",
      "           9.8159e-01, -2.2154e-01,  1.2828e+00,  5.1169e-01, -9.9675e-01,\n",
      "          -1.1644e-01,  3.2723e-02,  6.4577e-01, -1.2188e+00, -1.0159e+00,\n",
      "          -9.6017e-02],\n",
      "         [ 2.1672e-01, -2.2424e-01,  7.2139e-01,  9.2964e-01,  8.2497e-03,\n",
      "           6.0451e-01,  3.9639e-01,  5.8153e-01, -4.4550e-01,  4.7399e-01,\n",
      "           1.1575e-01, -2.5904e-01,  2.4590e-01, -1.4836e+00, -6.9498e-01,\n",
      "          -3.3072e-02],\n",
      "         [ 5.4385e-01, -3.9466e-01, -1.0125e+00, -5.4512e-01,  4.5627e-01,\n",
      "          -6.2368e-01, -4.4399e-01, -4.2563e-01, -2.7224e-01, -5.8117e-01,\n",
      "          -4.4086e-01, -1.7573e-01, -9.6186e-01,  6.8942e-01,  9.2475e-01,\n",
      "           1.0684e-01],\n",
      "         [ 3.3894e-01, -3.2556e-01,  4.6166e-01, -8.2820e-01,  4.4270e-01,\n",
      "          -1.4151e-01, -4.1609e-02,  1.9212e+00, -8.9523e-01, -5.0201e-01,\n",
      "          -1.1137e+00,  1.0437e+00, -1.7586e-01, -2.3682e-01,  9.3380e-01,\n",
      "          -4.4839e-01],\n",
      "         [-6.8397e-01, -8.4383e-01,  9.4528e-01,  1.4160e-01,  4.5911e-01,\n",
      "           1.1847e-01, -2.2957e-01,  2.7488e-01,  8.6874e-01,  4.7024e-01,\n",
      "          -8.9547e-01, -4.0157e-02,  1.1235e-01, -4.4020e-01, -5.2247e-01,\n",
      "           9.0527e-01],\n",
      "         [-3.9797e-01,  6.1179e-01,  8.2540e-01,  5.2400e-01,  3.5798e-02,\n",
      "           8.6325e-01, -4.4899e-02, -8.5363e-01,  2.4525e-01, -4.3185e-02,\n",
      "          -7.0380e-02,  1.9822e-01,  1.5965e+00,  8.8640e-03, -3.7233e-01,\n",
      "          -1.3505e-01],\n",
      "         [ 7.9026e-01, -4.1725e-01,  4.4386e-01,  4.0055e-01,  5.5536e-01,\n",
      "          -2.2373e-01,  2.9473e-01, -5.9832e-01, -1.6665e-01,  1.1418e-01,\n",
      "          -2.9226e-01, -5.2175e-01,  2.7492e-01, -7.8843e-01, -3.2329e-01,\n",
      "           5.0263e-01],\n",
      "         [ 3.4647e-01, -1.5322e+00,  3.5030e-01, -1.1737e+00,  5.7248e-01,\n",
      "          -1.7835e-01,  1.1274e-02, -4.6216e-02,  3.0792e-01,  2.0172e-01,\n",
      "          -1.7284e-02,  7.0737e-01,  1.6783e-01,  5.1844e-01,  1.0519e+00,\n",
      "           8.2206e-02]],\n",
      "\n",
      "        [[ 1.2819e+00,  5.6618e-01,  3.0633e-01, -1.7949e-01,  3.9011e-02,\n",
      "           4.0370e-01,  6.3128e-01, -9.2027e-02, -1.6131e+00, -1.7019e+00,\n",
      "          -4.2410e-01, -1.5391e-03, -7.7295e-02,  4.4488e-01,  4.4292e-02,\n",
      "          -3.5620e-04],\n",
      "         [-6.8673e-01,  1.1281e-01,  1.0612e+00,  5.7848e-01,  7.7326e-01,\n",
      "           1.2451e-01,  1.4613e+00, -2.6788e-01, -2.7784e-01, -5.2148e-01,\n",
      "          -9.2788e-02, -9.4883e-01, -4.6564e-02, -7.8491e-01, -6.5007e-01,\n",
      "           5.5048e-03],\n",
      "         [ 4.0858e-01, -1.4657e+00,  4.7286e-01, -2.5165e-01,  7.1570e-01,\n",
      "          -1.3565e+00, -5.1976e-01, -8.0625e-01,  3.1262e-01, -3.5172e-01,\n",
      "           3.5617e-01, -2.0855e-01, -7.5197e-01,  1.0913e+00,  2.5821e-01,\n",
      "           1.0210e-01],\n",
      "         [ 5.3702e-01,  1.8004e-01,  2.7467e-01,  7.2169e-01,  4.7249e-01,\n",
      "           1.2292e+00,  8.2438e-01, -3.5129e-01, -9.7774e-01,  2.4048e-02,\n",
      "          -1.8699e-01, -3.3693e-01,  4.4439e-01,  6.7698e-01,  3.5789e-01,\n",
      "          -6.5326e-01],\n",
      "         [-2.5759e-01,  9.5750e-01,  1.4890e+00, -4.4263e-01,  7.2383e-01,\n",
      "          -1.2631e+00, -1.4550e+00, -1.8996e-01,  9.9486e-02, -2.0744e-02,\n",
      "           5.1707e-01, -1.5242e-01, -2.1993e-01,  1.7348e+00,  4.1233e-01,\n",
      "           5.5115e-01],\n",
      "         [-3.5877e-01, -1.3932e-01, -5.7993e-01, -3.6010e-01,  1.8781e-01,\n",
      "           1.3696e+00,  3.3787e-01,  1.0588e+00,  1.2037e+00,  7.7723e-01,\n",
      "           2.1811e-01,  5.3820e-01,  4.9202e-02, -7.6328e-01,  8.2475e-01,\n",
      "          -2.4126e-01],\n",
      "         [ 1.0757e-01,  1.0222e+00,  1.1299e+00, -3.3066e-01,  5.5225e-01,\n",
      "          -1.7101e-01, -4.3461e-01, -9.1957e-01, -4.8826e-01, -1.0614e-01,\n",
      "           4.6397e-02,  1.9942e-01,  9.8178e-01,  5.2526e-01, -2.4993e-01,\n",
      "           6.6065e-01],\n",
      "         [ 3.8475e-01, -3.1977e-01, -2.9844e-01,  1.0349e+00, -7.3399e-01,\n",
      "           2.6185e-01,  9.9535e-01,  3.0518e-01, -5.8347e-02, -2.1505e-01,\n",
      "           2.8854e-02, -1.6734e-01,  2.5435e-01, -6.7979e-02,  1.3774e-01,\n",
      "          -1.3449e+00]],\n",
      "\n",
      "        [[ 1.5286e-02, -2.3789e-02, -5.9228e-01, -7.8481e-01, -1.2258e+00,\n",
      "           1.2360e+00,  1.2661e+00,  5.1090e-01, -1.0551e-02,  3.5479e-01,\n",
      "          -3.6292e-01, -7.8959e-01, -4.1690e-01, -1.1467e+00, -3.1735e-01,\n",
      "           4.7904e-01],\n",
      "         [ 3.6910e-02,  5.1398e-01, -5.0604e-01,  8.6663e-01, -7.6251e-02,\n",
      "           2.3465e-01,  6.6999e-01,  1.2919e-01, -5.7874e-01, -1.3623e+00,\n",
      "           6.5515e-01,  2.4927e-02, -2.1454e-01, -1.0696e+00, -1.6389e-01,\n",
      "          -3.0268e-01],\n",
      "         [-1.3370e-01, -4.3522e-01, -1.1573e+00, -2.4540e-01, -7.4640e-01,\n",
      "          -1.3177e-01,  7.6347e-01,  1.9822e-01, -3.0544e-01,  5.1263e-01,\n",
      "          -5.5005e-01,  2.1473e-01, -1.0727e+00, -2.6862e-01, -9.5364e-02,\n",
      "           7.5802e-01],\n",
      "         [ 2.7987e-01,  4.7265e-01, -5.1837e-01,  2.4377e-01,  8.4396e-01,\n",
      "           4.5062e-01,  3.2617e-01,  7.2392e-01, -7.9949e-01,  4.6918e-01,\n",
      "          -3.3891e-02, -1.9835e-01,  1.8117e-01, -3.3954e-01,  8.2529e-01,\n",
      "          -1.0077e-01],\n",
      "         [ 5.0776e-01,  7.0976e-01,  1.3320e+00, -1.3745e-01,  1.7298e-01,\n",
      "           5.7561e-01, -3.6395e-01,  3.9552e-01,  8.8274e-01,  2.1641e-01,\n",
      "          -9.5129e-01,  2.4206e-01,  2.0641e-01,  9.1389e-01,  1.1236e-01,\n",
      "          -2.1187e-01],\n",
      "         [ 8.0654e-02, -7.3005e-01, -2.7207e-01, -2.5438e-01,  2.9562e-02,\n",
      "           7.3331e-01,  6.0474e-01, -1.4401e-01, -8.2874e-01,  5.6843e-01,\n",
      "           3.9321e-01, -1.2027e-01,  3.8487e-01, -5.3320e-03,  2.6535e-01,\n",
      "          -3.2375e-01],\n",
      "         [-8.8515e-01,  5.1754e-01,  1.0735e+00, -3.6028e-01,  1.6631e-01,\n",
      "          -2.1409e-01, -8.0081e-01, -3.5068e-01, -5.5577e-01,  3.4054e-01,\n",
      "           3.2962e-02, -4.0673e-01,  6.3682e-01,  9.1886e-01,  4.1625e-01,\n",
      "           8.9215e-01],\n",
      "         [ 7.3551e-02,  7.6137e-01, -1.3097e+00,  3.3107e-01, -4.4359e-01,\n",
      "           8.2169e-02, -1.1593e+00, -6.7249e-01, -9.5755e-03, -2.5367e-01,\n",
      "          -8.2651e-01,  5.7297e-01,  1.3268e+00,  1.6529e+00, -2.8028e-01,\n",
      "           2.4315e-01]]], grad_fn=<UnsafeViewBackward0>)\n",
      "out[0]=tensor([[ 0.8247, -0.3350, -0.8547,  0.1472, -1.1222,  0.7396, -0.3802,  0.0115,\n",
      "          0.0116,  0.4242, -1.0590,  1.2839, -0.2837, -0.5439,  0.2245,  0.5877],\n",
      "        [ 0.8026, -0.2918, -0.7549,  0.1246, -0.9879,  0.7141, -0.3350,  0.0099,\n",
      "          0.0400,  0.4270, -0.9685,  1.1577, -0.2916, -0.4533,  0.2632,  0.5606],\n",
      "        [ 0.7287, -0.1760, -0.4768,  0.0861, -0.6054,  0.6113, -0.1993, -0.0094,\n",
      "          0.0971,  0.4195, -0.6931,  0.7533, -0.3077, -0.1981,  0.3387,  0.4734],\n",
      "        [ 0.3273, -0.2890, -0.4273,  0.6600, -0.2665, -0.3937,  0.0770, -0.4304,\n",
      "         -0.4226, -0.0015,  0.0599, -0.9384, -0.1423,  0.0043, -0.5961,  0.0962],\n",
      "        [ 0.3801,  0.6762,  0.8038,  0.2665,  0.3693,  0.6216, -0.3815, -0.4026,\n",
      "          0.1745,  0.0558, -0.6313,  0.5332,  0.2049,  0.0173, -0.4128, -0.2607],\n",
      "        [ 0.3031, -0.1174, -0.2080,  0.6792, -0.0882, -0.3128,  0.0683, -0.4622,\n",
      "         -0.3859, -0.0545,  0.0401, -0.8880, -0.0805,  0.0307, -0.5999,  0.0264],\n",
      "        [ 0.2383, -0.1754, -0.6259, -0.0288, -0.4157, -0.0158, -0.0530, -0.1201,\n",
      "          0.2100,  0.0514, -0.1900, -0.1193, -0.2089,  0.0919, -0.0230,  0.0589],\n",
      "        [ 0.8094, -0.3206, -0.8402,  0.1423, -1.0922,  0.7197, -0.3766,  0.0072,\n",
      "          0.0222,  0.4111, -1.0371,  1.2452, -0.2758, -0.5213,  0.2139,  0.5693]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# single head self-attention\n",
    "head_size = 16 # number of features (H)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# Every node (token) emits two vectors:\n",
    "# 1. query (Q) - what a token is looking for\n",
    "# 2. key (k) - what a token is\n",
    "q = query(x) # (B, T, H)\n",
    "k = key(x)   # (B, T, H)\n",
    "\n",
    "# Transpose the last two dimensions of k -> (B, H, T)\n",
    "# (B, T, H) @ (B, H, T) -> (B, T, T)\n",
    "ws = q @ k.transpose(-2, -1)\n",
    "\n",
    "# only pay attention to preceding tokens\n",
    "tril = torch.tril(torch.ones(T, T)) # (T, T)\n",
    "# zeroes mask (T, T) broadcast through weights (B, T, T)\n",
    "ws = ws.masked_fill(tril == 0, float('-inf')) # (B, T, T)\n",
    "# normalise\n",
    "ws = F.softmax(ws, dim=-1) # (B, T, T)\n",
    "print(f\"softmaxed: {ws=}\")\n",
    "\n",
    "# the content that a token has to offer\n",
    "v = value(x) # (B, T, H)\n",
    "\n",
    "# weighted aggregation of past tokens\n",
    "out = ws @ v # (B, T, H)\n",
    "\n",
    "print(f\"{v=}\")\n",
    "print(f\"{out[0]=}\")\n",
    "\n",
    "# attention weights of tokens in first sequence\n",
    "# ws[0] # (T, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Single head of self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"A head of self attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size: int):\n",
    "        super().__init__()\n",
    "        # Initialise Q, K, V\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        # the mask doesn't get updated during backpropagation (optimizer step)\n",
    "        # but is part of the state of the model, so saved and loaded with the model\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B, T, C = x.shape\n",
    "        q = self.query(x) # (B, T, C)\n",
    "        k = self.key(x)   # (B, T, C)\n",
    "\n",
    "        # compute attention scores (\"affinities\")\n",
    "        ws = q @ k.transpose(-2, -1) # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        ws = ws.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "\n",
    "        # compute relative predictions\n",
    "        ws = F.softmax(ws, dim=-1) # (B, T, T)\n",
    "\n",
    "        v = self.value(x) # (B, T, C)\n",
    "        out = ws @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Multi-head self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple heads of self-attention concatenated.\n",
    "    Parallelized over the channel (C) dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads: int, head_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # initialise multiple self-attention heads each with `head_size` dimension\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "        # projection layer to transform concatenated outputs of all heads\n",
    "        # back to the original embedding dimension\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # concat outputs of each attention head\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "        # project outputs to input (embed) dimensions\n",
    "        out = self.proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Feed forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple feed-forward NN with a single hidden layer\n",
    "    [Linear -> ReLU]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int):\n",
    "        super().__init__()\n",
    "        # scaling inner layer channel size by 4 is a hyperparameter recommendation in\n",
    "        # 3.3 Position-wise feed forward networks from Attention is All You Need\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), # projection\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Carries out self attention (communication) and feed forward (computation) on input embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, n_heads: int):\n",
    "        \"\"\"\n",
    "        :param n_embd: embedding dimension\n",
    "        :param n_heads: number of heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # self-attention heads\n",
    "        # the same 32 embedding dimensions are split across 4 heads\n",
    "        head_size: int = n_embd // n_heads\n",
    "        self.sa = MultiHead(n_heads, head_size)\n",
    "\n",
    "        # feed forward layer\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "        # both LayerNorm layers\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Applies a round of self-attention to embeddings.\n",
    "        :param x: the current embeddings with shape (B, T, C)\n",
    "        :return: output data following self-attention\n",
    "        \"\"\"\n",
    "\n",
    "        # apply self attention to normalised plus original x for residual connection\n",
    "        # using pre-norm formulation here (modern deviation from Attention is All You Need)\n",
    "        norm_x = self.ln1(x)\n",
    "        x = x + self.sa(norm_x)\n",
    "\n",
    "        # apply feed forward plus original x for residual connection\n",
    "        norm_x = self.ln2(x)\n",
    "        x = x + self.ffwd(norm_x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# number of embedding dimensions\n",
    "n_embd = 32\n",
    "\n",
    "class DecoderLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*(TransformerBlock(n_embd, n_heads=n_heads) for _ in range (n_block_layers)))\n",
    "        self.ln_f = nn.LayerNorm(n_embd), # last layer normalisation\n",
    "\n",
    "        # language-modelling head\n",
    "        # maps output of transformer block (self-attention and feed-forward layers) to the vocabulary\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idxs: torch.Tensor, targets=None):\n",
    "\n",
    "        B, T = idxs.shape\n",
    "\n",
    "        # Sum text and position information\n",
    "        tok_emb = self.token_embedding_table(idxs)                              # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "\n",
    "        # sum token embeddings with broadcast position embeddings, right-aligned as (1, T, C)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            # inference mode -> we don't need to calculate loss\n",
    "            loss = None\n",
    "        else:\n",
    "            # training mode -> we need to calculate loss\n",
    "\n",
    "            # negative log likelihood loss\n",
    "            # pytorch wants B x T x C\n",
    "            B, T, C = logits.shape\n",
    "            # print(f\"B, T, C = {logits.shape=}\")\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T) # or .view(-1)\n",
    "\n",
    "            # Compute loss between logit and target tensors\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idxs, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Inference.\n",
    "        Turns a (B, T) array of indices into (B, T+1), (B, T+2)... (B, T+N)\n",
    "        :param idx: a (B, T) array of indices in the current context\n",
    "        :param max_new_tokens:\n",
    "        :return: a (B, T+max_new_tokens) array of indices\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop tokens that fit into context\n",
    "            cropped_idxs = idxs[:, -block_size:]\n",
    "            # get predictions\n",
    "            logits, loss = self(cropped_idxs)\n",
    "            # take the logits for the last token in the current sequence\n",
    "            logits = logits[:, -1, :] # becomes (B, C), taking the last in the T dimension\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from distribution\n",
    "            next_ix = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idxs = torch.cat((idxs, next_ix), dim=1) # (B, T+1)\n",
    "        return idxs\n",
    "\n",
    "\n",
    "def test_model():\n",
    "    m = DecoderLanguageModel()\n",
    "    logits, loss = m(xb, yb)\n",
    "    print(f\"logits: {logits.shape}\")\n",
    "    print(loss.item())\n",
    "\n",
    "    context = torch.zeros([1, 1], dtype=torch.long, device=device)\n",
    "    print(decode(m.generate(context, max_new_tokens=100)[0].tolist()))\n",
    "\n",
    "\n",
    "test_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepare to estimate loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            X, Y = get_batch(is_training=split == 'train')\n",
    "            logits, loss = m(X, Y)\n",
    "            losses[i] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(model: nn.Module, train_steps=10_000):\n",
    "    loss_values = []\n",
    "    loss = None\n",
    "    for step in range(train_steps):\n",
    "\n",
    "        # Sample a batch of data for training. `xb` contains the input sequences and `yb` contains the target sequences.\n",
    "        xb, yb = get_batch(is_training=True)\n",
    "\n",
    "        # Forward pass: Compute predictions (logits) and loss for this batch using the model.\n",
    "        # loss= L(yb, logits) where L is loss function (cross-entropy)\n",
    "        logits, loss = model(xb, yb)\n",
    "\n",
    "        # Before computing the gradients, we need to zero out the gradients from the previous step.\n",
    "        # This is because PyTorch accumulates gradients on subsequent backward passes.\n",
    "        # Set gradients of all params to zero: ∇params = 0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Backward pass: Compute the gradients of the loss with respect to the model's parameters.\n",
    "        # ∇params = d(loss)/d(params)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model's parameters using the computed gradients.\n",
    "        # params' = params - learning_rate * ∇params\n",
    "        optimizer.step()\n",
    "\n",
    "        # store the detached loss value for this step\n",
    "        loss_values.append(loss.detach().item())\n",
    "\n",
    "        # print loss at intervals\n",
    "        if step % 500 == 0:\n",
    "            estimated_loss = estimate_loss()\n",
    "            print(f\"step {step} – training loss: {estimated_loss['train']} | val loss: {estimated_loss['val']}\")\n",
    "\n",
    "    # print loss for the last batch\n",
    "    # not necessarily the average or total loss for all batches.\n",
    "    if loss is not None:\n",
    "        print(f\"loss after {train_steps} training steps: {loss.detach().item()}\")\n",
    "    return loss_values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = DecoderLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# Initialize the optimiser with the parameters of the model `m` and a learning rate of 1e-3.\n",
    "# AdamW is a variant of the Adam optimiser that has weight decay regularization.\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) # learning rate of 1e-4 for bigger NNs\n",
    "\n",
    "train_steps = 10_000\n",
    "loss_values = train(model=m, train_steps=train_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analyse loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def smooth(values, beta):\n",
    "    smoothed = []\n",
    "    for v in values:\n",
    "        if smoothed:\n",
    "            previous = smoothed[-1]\n",
    "            smoothed.append(previous * beta + (1 - beta) * v)\n",
    "        else:\n",
    "            smoothed.append(v)\n",
    "    return smoothed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_colour = '#1f77b4'\n",
    "\n",
    "# plot loss values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_values, label=f\"{train_steps} steps\", color=plot_colour, alpha=.2)\n",
    "plt.plot(smooth(loss_values, 0.995), label=f\"{train_steps} steps\", color=plot_colour)\n",
    "plt.xlabel('training step')\n",
    "plt.ylabel('loss')\n",
    "plt.title('loss vs training steps')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.tensor(encode(\"I'm from the \"), dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.zeros([1, 1], dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idx = torch.tensor([encode(\"You used to call me on my \")], dtype=torch.long) # we're going to need more Drake albums\n",
    "print(decode(m.generate(idx, max_new_tokens=300)[0].tolist()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([37,  7, 70,  1, 63, 75, 72, 70,  1, 77, 65, 62,  1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(encode(\"I'm from the \"), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros([1, 1], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You used to call me on my lone\n",
      "E'It goinf\n",
      "Girls\n",
      "Sound\n",
      "And the slike 11: Drake]\n",
      "But anyy\n",
      "\n",
      "[Putro: Drake you Godeach\n",
      "With neds poplies whit\n",
      ", yeah, it's vechack's)\n",
      "Nowe]\n",
      "Appich, Homes me all never becone, svilly hobuld it's when I'm felus?\n",
      "They Bare\n",
      "\n",
      "[Verse 1: Eem the giivice, you\"\"\n",
      "\n",
      "[Ponerst\"\n",
      "I'm niggre it I onees be uppink\n",
      "B\n"
     ]
    }
   ],
   "source": [
    "idx = torch.tensor([encode(\"You used to call me on my \")], dtype=torch.long) # we're going to need more Drake albums\n",
    "print(decode(m.generate(idx, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for param in m.parameters():\n",
    "#     print(f\"{type(param)=} - {param=}: {param.size()=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Expected loss (without training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "print(f\"{-math.log(1/len(chars))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}